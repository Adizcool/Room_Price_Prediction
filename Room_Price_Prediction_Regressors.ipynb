{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Room_Price_Prediction_Regressors.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Adizcool/Room_Price_Prediction/blob/main/Room_Price_Prediction_Regressors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euOA_1A3Mj9S"
      },
      "source": [
        "## **Using all the Regressors with Advertising Dataset**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFy-Sxv-Mj9U"
      },
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sb\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrk5NIaF83M0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e1e0f6e-489f-4a0f-b4f0-ab2308f56b4b"
      },
      "source": [
        "# Mount the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcY63z_wMj9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "384434ce-2551-48f0-858b-e61235ec6ea7"
      },
      "source": [
        "# Importing the dataset\n",
        "dataset = pd.read_csv(\"gdrive/My Drive/Datasets/Room Price Prediction/Room_Price_Data_Preprocessed.csv\")\n",
        "print(dataset.shape)\n",
        "dataset.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4201, 22)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CancellationPolicy</th>\n",
              "      <th>Accommodates</th>\n",
              "      <th>RoomType</th>\n",
              "      <th>Bathrooms</th>\n",
              "      <th>Bedrooms</th>\n",
              "      <th>CleaningFee</th>\n",
              "      <th>ReviewRating</th>\n",
              "      <th>Price</th>\n",
              "      <th>District_Brooklyn</th>\n",
              "      <th>District_Manhattan</th>\n",
              "      <th>District_Queens</th>\n",
              "      <th>Neighborhood_0</th>\n",
              "      <th>Neighborhood_1</th>\n",
              "      <th>Neighborhood_2</th>\n",
              "      <th>Neighborhood_3</th>\n",
              "      <th>Neighborhood_4</th>\n",
              "      <th>Neighborhood_5</th>\n",
              "      <th>PropertyType_0</th>\n",
              "      <th>PropertyType_1</th>\n",
              "      <th>PropertyType_2</th>\n",
              "      <th>PropertyType_3</th>\n",
              "      <th>PropertyType_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>75.000000</td>\n",
              "      <td>86.0</td>\n",
              "      <td>160.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>66.646525</td>\n",
              "      <td>100.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>91.0</td>\n",
              "      <td>129.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>160.000000</td>\n",
              "      <td>94.0</td>\n",
              "      <td>399.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   CancellationPolicy  Accommodates  ...  PropertyType_3  PropertyType_4\n",
              "0                   2             2  ...               0               1\n",
              "1                   0             2  ...               0               1\n",
              "2                   2             4  ...               0               1\n",
              "3                   0             2  ...               0               1\n",
              "4                   2             9  ...               0               1\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu_PQCb3vA2u",
        "outputId": "67f08d6b-9f09-4cdf-e872-24403badbf74"
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4201 entries, 0 to 4200\n",
            "Data columns (total 22 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   CancellationPolicy  4201 non-null   int64  \n",
            " 1   Accommodates        4201 non-null   int64  \n",
            " 2   RoomType            4201 non-null   int64  \n",
            " 3   Bathrooms           4201 non-null   float64\n",
            " 4   Bedrooms            4201 non-null   float64\n",
            " 5   CleaningFee         4201 non-null   float64\n",
            " 6   ReviewRating        4201 non-null   float64\n",
            " 7   Price               4201 non-null   float64\n",
            " 8   District_Brooklyn   4201 non-null   int64  \n",
            " 9   District_Manhattan  4201 non-null   int64  \n",
            " 10  District_Queens     4201 non-null   int64  \n",
            " 11  Neighborhood_0      4201 non-null   int64  \n",
            " 12  Neighborhood_1      4201 non-null   int64  \n",
            " 13  Neighborhood_2      4201 non-null   int64  \n",
            " 14  Neighborhood_3      4201 non-null   int64  \n",
            " 15  Neighborhood_4      4201 non-null   int64  \n",
            " 16  Neighborhood_5      4201 non-null   int64  \n",
            " 17  PropertyType_0      4201 non-null   int64  \n",
            " 18  PropertyType_1      4201 non-null   int64  \n",
            " 19  PropertyType_2      4201 non-null   int64  \n",
            " 20  PropertyType_3      4201 non-null   int64  \n",
            " 21  PropertyType_4      4201 non-null   int64  \n",
            "dtypes: float64(5), int64(17)\n",
            "memory usage: 722.2 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb4owSB4A9sC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "cc51c3ad-f1ed-49f1-f85d-4113359c366d"
      },
      "source": [
        "X=dataset.drop(['Price'], axis=1)\n",
        "y=dataset['Price']\n",
        "X[:5]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CancellationPolicy</th>\n",
              "      <th>Accommodates</th>\n",
              "      <th>RoomType</th>\n",
              "      <th>Bathrooms</th>\n",
              "      <th>Bedrooms</th>\n",
              "      <th>CleaningFee</th>\n",
              "      <th>ReviewRating</th>\n",
              "      <th>District_Brooklyn</th>\n",
              "      <th>District_Manhattan</th>\n",
              "      <th>District_Queens</th>\n",
              "      <th>Neighborhood_0</th>\n",
              "      <th>Neighborhood_1</th>\n",
              "      <th>Neighborhood_2</th>\n",
              "      <th>Neighborhood_3</th>\n",
              "      <th>Neighborhood_4</th>\n",
              "      <th>Neighborhood_5</th>\n",
              "      <th>PropertyType_0</th>\n",
              "      <th>PropertyType_1</th>\n",
              "      <th>PropertyType_2</th>\n",
              "      <th>PropertyType_3</th>\n",
              "      <th>PropertyType_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>75.000000</td>\n",
              "      <td>86.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>66.646525</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>69.000000</td>\n",
              "      <td>91.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>160.000000</td>\n",
              "      <td>94.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   CancellationPolicy  Accommodates  ...  PropertyType_3  PropertyType_4\n",
              "0                   2             2  ...               0               1\n",
              "1                   0             2  ...               0               1\n",
              "2                   2             4  ...               0               1\n",
              "3                   0             2  ...               0               1\n",
              "4                   2             9  ...               0               1\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTbMT5PFvdjH",
        "outputId": "e9f79450-85ed-4a92-d78d-c65ea2ddfce9"
      },
      "source": [
        "y[:5]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    160.0\n",
              "1    130.0\n",
              "2    129.0\n",
              "3    145.0\n",
              "4    399.0\n",
              "Name: Price, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YSLWco_Mj-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe601bc5-f925-4bdf-a316-5c42aceeff29"
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3360, 21)\n",
            "(3360,)\n",
            "(841, 21)\n",
            "(841,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5ZcHUa1fg2j"
      },
      "source": [
        "##**1) Linear Regressor (LR)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQIpiiUPMj_M"
      },
      "source": [
        "# Fitting Multiple Linear Regression to the Training set\n",
        "from sklearn.linear_model import LinearRegression\n",
        "LR = LinearRegression()\n",
        "model=LR.fit(X_train, y_train)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-f7_VUwITc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e41ce699-bb6f-4815-b83d-f5aa29499804"
      },
      "source": [
        "# Calculate the accuracy of learning by the Algorithm\n",
        "LR.score(X_train,y_train) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5713014954226205"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9Lt-ZSNMj_X"
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = LR.predict(X_test)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GktxUZe7IkGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97a34eaf-449e-4230-86dc-0f813ccda666"
      },
      "source": [
        "# Calcualte the accuracy of prediction (y_test Vs y_pred) # This measure is part of Model Evaluation\n",
        "r2_score(y_test, y_pred)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5505049463642514"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqd4dhfDJBa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a012a868-475e-446d-9b59-63ce9c9a1ba2"
      },
      "source": [
        "# Calculate the Model Error (RMSE) # This measure is also part of Model Evaluation\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "rmse"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71.89127826709701"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICULTYRj7J47"
      },
      "source": [
        "LR_Training_Acc= LR.score(X_train,y_train)\n",
        "LR_Testing_Acc = r2_score(y_test,y_pred)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J4H4DMT7X0I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ab4d921-2060-4724-8f36-f0f3c93a434a"
      },
      "source": [
        "print(\"Training Accuracy :\", LR_Training_Acc)\n",
        "print(\"Testing Accuracy :\", LR_Testing_Acc)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy : 0.5713014954226205\n",
            "Testing Accuracy : 0.5505049463642514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EWPQ6q_OimA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "908b5ace-d833-4714-fe55-4b779d63acd5"
      },
      "source": [
        "New_prediction1 = LR.predict(X_train)\n",
        "print(New_prediction1[:5])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[203.52566778 198.97249805 113.3094839  168.4206496  172.25001825]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6tmxMVf7zmZ"
      },
      "source": [
        "LR_Training_Err= mean_squared_error(y_train,New_prediction1)\n",
        "LR_Testing_Err = mean_squared_error(y_test, y_pred)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76c5G__P8K4A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2feb04fb-4fe5-4536-b1c5-9f05078c8b49"
      },
      "source": [
        "print(\"Training Error :\", LR_Training_Err)\n",
        "print(\"Testing Error :\", LR_Testing_Err)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Error : 5292.042320787496\n",
            "Testing Error : 5168.355890877175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPf6quh5Mr_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8625b5c6-f0c5-4c88-dfb9-35b6f586babb"
      },
      "source": [
        "# k-fold CV \n",
        "#lm = LinearRegression()\n",
        "scores1 = cross_val_score(LR, X, y, scoring='r2', cv=5)\n",
        "print(scores1)\n",
        "LR_CV_Average=np.average(scores1)   \n",
        "LR_CV_Average"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.57392339 0.50851993 0.60207924 0.52118282 0.58972781]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5590866382261763"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZuvu4JuLudT"
      },
      "source": [
        "##**2) Random Forest Regressor (RFR)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHdE6ng1_N35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5cd9a77-c0e7-4fb0-a806-bab3a64e5169"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "RFR = RandomForestRegressor(n_estimators=300, random_state=0)\n",
        "RFR.fit(X_train, y_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(n_estimators=300, random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb7Wx8P_L2rD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da85817-ce1e-4d59-eeab-c006dc878b28"
      },
      "source": [
        "# Calculate the accuracy of learning by the Algorithm\n",
        "RFR.score(X_train,y_train) "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9330448761658652"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeX_zezIMIFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5bc3d8c-2011-4f44-d914-91173b99a301"
      },
      "source": [
        "# Predicting the Test set results\n",
        "RFR_y_pred = RFR.predict(X_test)\n",
        "print(y_test)\n",
        "print()\n",
        "print(RFR_y_pred)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3037    245.0\n",
            "3485    339.0\n",
            "3347     40.0\n",
            "72      200.0\n",
            "2284    200.0\n",
            "        ...  \n",
            "643      85.0\n",
            "570      68.0\n",
            "3509    175.0\n",
            "173     250.0\n",
            "1746    850.0\n",
            "Name: Price, Length: 841, dtype: float64\n",
            "\n",
            "[356.98       339.65        90.37388889 152.71511111 156.18283333\n",
            "  90.42333333 147.14       108.12333333  62.65        85.21362698\n",
            " 170.85       220.715      141.77888889  49.45410053 154.305\n",
            "  66.29333333 120.56333333  81.33277778 193.55777778  86.80722222\n",
            " 154.4         58.59863757  94.74722222 150.72666667 102.42666667\n",
            " 167.19166667 178.18066667 211.13061111  91.08333333 600.87333333\n",
            "  76.61166667  94.84       228.35566667  85.69370238 380.99333333\n",
            "  67.16       115.73111111 111.20333333 220.97888889 247.87055556\n",
            " 105.70166667  98.90722222  70.37        92.82988889 184.38666667\n",
            " 200.47427778  90.61638889 229.67666667 280.03555556 132.86166667\n",
            " 209.85638889 254.77333333  53.30611111 139.26777778 137.40805556\n",
            " 163.69833333 251.78916667 168.1216746  106.97638889 158.26666667\n",
            " 157.25666667 146.30333333 162.96833333 178.87277778 178.28444444\n",
            " 250.46       182.27144444 126.335       94.71128968 130.40311111\n",
            " 110.8        120.6        139.03       134.99       583.86333333\n",
            " 243.29333333 104.19222222 184.1        487.40666667 170.77133333\n",
            " 300.03666667  77.33266667 481.88333333 229.93666667 396.82\n",
            " 304.19666667 258.76       172.24       214.43066667 256.55166667\n",
            " 161.02777778 221.45666667 253.36888889 175.86338889 209.33\n",
            "  90.16522222 109.52888889 211.11166667 119.71638889 158.41333333\n",
            "  67.19833333 180.69       217.97833333 511.61333333 104.26666667\n",
            "  76.29461111 185.88527778  89.07        74.17511111 181.90444444\n",
            " 199.69666667  64.81222222  74.25791667 119.72916667 335.70559524\n",
            " 156.17361111  88.07       157.85666667 200.22        95.42666667\n",
            " 183.32111111 234.67055556 113.70333333  85.05831746 229.22666667\n",
            " 285.96666667 118.62       176.58142196 200.57593122 228.02620635\n",
            " 137.25       118.56790476  83.39       105.1847619  130.146\n",
            " 206.87511111 363.52666667 178.70111111  86.81       112.10833333\n",
            " 104.43944444 130.28333333 137.84333333 335.72333333 124.93833333\n",
            " 264.76666667 209.45833333  72.16988889 252.09388889 159.89388889\n",
            " 147.14166667 403.17666667  77.02819444 113.41888889  84.25547619\n",
            " 107.36666667  74.97666667 398.33666667 175.36305556 116.26542857\n",
            " 109.12408886 179.         164.28583333 198.12333333 175.96666667\n",
            " 229.337      514.79666667 213.70927778  76.29666667 172.08666667\n",
            " 181.87666667  95.24083333 336.34333333 108.14694444  96.26666667\n",
            " 166.52666667 177.28444444  94.80888889 122.1375     136.63230159\n",
            "  70.77150794  99.61444444 129.01        67.36       132.33333333\n",
            " 141.52333333 109.46633333  92.90222222  90.10277778 484.58333333\n",
            " 196.86961111 198.66333333  56.665      138.05333333 154.29416667\n",
            "  95.99333333 108.24475397 202.04       336.99666667  82.75722222\n",
            " 194.02672222 103.92516667 103.92940476 181.33166667  80.58395238\n",
            " 136.47333333  93.165      307.51666667  81.64694444 101.23396825\n",
            "  78.85916667  64.33238095 162.73788889 184.56222222 103.20666667\n",
            " 195.06333333 201.90333333 477.05333333 119.45366667 554.93\n",
            "  73.13666667 261.82333333  68.49194577 180.79333333 215.90061111\n",
            " 207.72724074 196.98222222 154.15883333 102.13       344.88333333\n",
            "  86.48333333  85.4175     264.79961111  78.32244444 116.65833333\n",
            " 195.54888889  90.86444444 216.16416667 102.26055556  59.93822222\n",
            "  81.30666667 224.63666667 189.00666667  71.75055556 244.59\n",
            "  48.12666667 128.14247222 117.19055556  65.59444444 115.26333333\n",
            "  68.99333333 122.26716667  67.20333333 112.97538889  81.50247619\n",
            " 164.73388889 221.51       292.10333333 160.14611111 112.93666667\n",
            " 235.93111111  77.57        84.55283333 157.16       212.22555556\n",
            " 202.48344444  93.68333333 412.24333333 115.295      154.87\n",
            " 157.704      149.44333333 164.63       383.88        90.9225\n",
            " 171.34185185 128.48666667 246.50333333 525.81666667  68.64666667\n",
            " 172.65138889 169.08333333  98.86       164.84333333  96.37555556\n",
            " 224.94666667 280.55333333 158.09       114.08622222 126.335\n",
            "  93.84666667 431.09333333 236.31333333  91.03696693  91.97402778\n",
            " 183.99333333 152.67444444 320.88388889 286.84333333 285.21\n",
            " 138.42833333 137.36       186.18035714 174.56833333  86.64583333\n",
            " 217.225      258.54666667  90.12513228 145.51664286  57.22333333\n",
            " 485.42        74.32060979 104.25       168.29       108.88546296\n",
            " 212.05255556 324.78913889 180.38166667 259.71333333 164.07430952\n",
            " 157.665       68.89       294.65666667  58.78888889 161.75688889\n",
            " 445.40333333 296.38666667 158.412      211.69333333 180.32744444\n",
            "  89.2331164  110.12011111 179.80333333  68.64666667 180.32666667\n",
            "  73.53166667 426.13333333 100.03883333 132.49105556 151.20611111\n",
            "  97.1790873  194.73722222 108.23       423.11444444  75.23266667\n",
            " 246.20555556 128.57577778  58.12333333 104.01666667  78.74719841\n",
            " 152.18087302 124.24666667  94.48083333  65.94555556 204.57888889\n",
            " 201.98        80.15       128.2575      84.4584709   73.34211905\n",
            " 159.688      101.87139683 153.66083333 194.68       180.05666667\n",
            " 149.24        91.58        89.60666667 300.81333333 134.94666667\n",
            " 541.91333333 204.12333333 182.937      160.875      187.08294444\n",
            " 230.09603175 184.20880952 116.55666667  69.50666667 161.22555556\n",
            "  78.86806481 354.83666667  61.36834921 135.92455556 103.73638889\n",
            " 224.76833333  84.585       68.75277778  98.73866534 169.125\n",
            " 162.39313095 183.17155556 169.65       231.29904762 445.19666667\n",
            " 149.73666667  87.59333333  93.28666667  85.64366667 205.24361111\n",
            " 174.32666667 422.52666667  78.69333333  70.95123016 195.93855556\n",
            " 131.76111111 106.14       171.1        483.98333333 223.47666667\n",
            " 132.11777778 174.12166667  74.25791667  80.22       158.39666667\n",
            " 132.07755556  69.47333333  78.76198413 202.46938095  96.01\n",
            " 440.62       226.05333333  83.47277778  78.88611111 336.32666667\n",
            " 197.41666667 181.49638889  67.02333333 168.78       194.37333333\n",
            "  68.08822222 213.37322222 136.09       162.08533333  69.02420635\n",
            " 107.33589947  78.37266667 233.42333333 100.92975     67.65215079\n",
            " 152.64111111 178.71480159 130.18333333 566.69333333 226.91333333\n",
            " 255.03       175.05833333 156.65888889 406.38666667 111.19\n",
            "  97.5         60.01666667  64.17333333 175.62633333 141.30333333\n",
            " 140.16286508 164.07430952 205.25444444  64.02323557 239.59666667\n",
            "  75.95933333  90.99666667 244.75666667  71.58944444 100.17861111\n",
            "  75.33       132.59154762 160.99666667 108.78284127 361.08333333\n",
            " 172.30583333 166.42555556 252.09055556 196.34666667 102.51666667\n",
            " 129.44277778 166.59269841 126.85        87.05766667  74.13611111\n",
            " 162.11666667 168.37        84.24095238 160.99666667 171.00977778\n",
            "  91.08344444 139.10527778 197.73111111 123.56333333 240.09333333\n",
            " 195.99974603  94.76131746  88.33694444 232.92333333 193.417\n",
            " 204.01       292.96333333 198.14607407  71.6075     144.79777778\n",
            " 149.69888889  70.86944444 188.23138889 179.41       120.55366667\n",
            " 113.13       247.75833333  76.94694444 140.83488889  92.41901587\n",
            " 150.16222222 598.11        82.65547619 364.05666667  74.13\n",
            " 251.70666667  87.53333333  75.6975     238.87055556 108.93665079\n",
            " 167.62555556 384.25333333  71.66616667 207.145      179.78\n",
            "  89.13833333 129.56244444  52.98205111 172.51777778 142.90672222\n",
            "  78.54       368.22666667  85.79666667 161.57930952 156.18255556\n",
            "  85.77587302  70.62166667 164.62       185.64166667 119.10083333\n",
            " 510.68666667 373.1        231.24333333  75.31622222  72.39666667\n",
            " 149.34622222  87.13597923 162.80777778 200.9        291.68333333\n",
            " 193.55730159 180.72722222 111.91555556 259.23       106.21\n",
            "  96.92063492 430.27333333 182.65738095 263.99        93.27833333\n",
            "  70.60488095 227.24666667 160.79106085  77.43       116.36545839\n",
            " 263.16        58.40807143 204.01333333 508.90333333 322.27\n",
            "  93.2125      72.22411111  56.17911195 130.45333333 197.65111111\n",
            " 103.92940476 110.73333333 166.48777778 245.60333333 173.81333333\n",
            " 209.70444444  68.40561111 190.84666667 180.09111111  97.34277778\n",
            " 152.44222222  75.39666667 117.42666667  68.71222222 145.67981746\n",
            "  95.34466667 107.54       191.06       120.3         50.99666667\n",
            " 196.75361111 113.31388889 124.70666667 173.96        69.48\n",
            " 218.69       180.98833333 114.73       132.49105556 153.97777778\n",
            " 218.64166667 162.15333333 164.27416667 192.88666667  67.05966667\n",
            " 130.5795      94.63666667  82.35833333 218.98555556 105.78333333\n",
            "  64.50916667 162.91559524 214.47333333  70.38025016 131.1275\n",
            " 213.53333333  90.29444444 232.11333333  82.77833333 234.02666667\n",
            "  83.41452381 236.58666667  90.16522222 115.30666667 112.88588889\n",
            " 128.14166667  60.62266667  85.4175      63.86666667 180.96111111\n",
            "  90.97055556  77.76466667  64.19972222 140.07666667 120.99333333\n",
            " 156.47694444 252.99666667 172.41355556  76.27574603 194.14571429\n",
            "  61.25666667 124.23333333 197.335      153.26825397  90.52666667\n",
            "  82.61833333  97.58888889 287.23       245.98       195.62007407\n",
            " 161.10777778 165.05222222 123.22733333 142.07466667 198.14607407\n",
            "  92.52333333 203.12333333  57.96567593 149.26333333 177.64083333\n",
            " 104.12388889  76.92676984  68.76955556 165.74366667 102.72333333\n",
            "  70.38025016 202.74851852 193.325      352.64        55.66242063\n",
            " 159.75005556 277.52333333 147.475       85.69183333 194.78714286\n",
            " 425.73666667 178.21738889  68.35083333 204.19333333 149.97\n",
            " 126.52611111  65.14777778  89.81644444 150.17333333 146.82333333\n",
            " 138.93666667 165.28666667 186.04333333 232.88666667 105.6572619\n",
            " 264.          76.76333333 107.075      145.67333333 226.90333333\n",
            " 132.60944444  87.41685185  99.68222222 127.0475     165.16277778\n",
            "  88.36666667 177.5135     115.82        97.41       353.88333333\n",
            " 147.96166667 128.89083333 113.79666667 224.94       204.443\n",
            " 217.50222222 155.51016667 173.368      204.36666667 235.095\n",
            " 160.04388889 265.99222222  78.27666667 267.39        93.59691667\n",
            " 375.62333333  77.318       71.876      193.60833333 176.95333333\n",
            " 100.07187037 150.38        91.06811111 198.16833333  80.97592593\n",
            " 186.105      146.79666667  88.55333333 213.29222222 184.145\n",
            " 347.38333333 145.82411111 168.87666667 242.45       135.94777778\n",
            " 186.34666667 237.90555556 188.96422222 181.14       185.219\n",
            " 158.62321429  83.70720635  86.94721782 106.90415873 288.85\n",
            " 100.21444444 193.417       90.39111111  76.94694444 143.40333333\n",
            " 413.33666667  67.37555556 220.89877778 199.47022222 112.6\n",
            "  78.96166667 176.83055556 258.67        77.24444444  97.93242063\n",
            "  76.76       174.91655556 168.83666667  99.68222222  87.715\n",
            "  97.92833333 151.10111111  99.43       136.08       190.24484127\n",
            " 130.10111111 133.05333333 103.7572175  103.07773016 132.87\n",
            " 203.77277778 119.98432407 260.99833333 266.77333333 261.41777778\n",
            " 213.72305556 124.68333333 116.88333333 147.74333333  77.98666667\n",
            " 166.99883333 140.45472222 162.848      227.16333333 222.06111111\n",
            " 204.46220899 209.85666667  73.89750794 126.64390476  76.29809524\n",
            " 206.45       165.92277778 127.97265079 183.99638889  65.665\n",
            " 200.48666667  89.74166667  71.75784127 175.49333333 158.13916667\n",
            " 204.475      171.68166667  88.35822222 314.03833333 246.30333333\n",
            " 106.13940476 119.05333333 106.46505556 201.91666667 112.09666667\n",
            " 187.81       212.65333333  76.91644444  91.82388889  83.04666667\n",
            " 117.33       120.55366667  83.3798373  172.84666667  64.45555556\n",
            " 572.86666667  73.26833333  76.83702381 203.11       255.85\n",
            " 257.05      ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRNKj1iH5xIG"
      },
      "source": [
        "RFR_Training_Acc = RFR.score(X_train,y_train)\n",
        "RFR_Testing_Acc = r2_score(y_test,RFR_y_pred)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3P9uL_b6WkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46b9dff8-57a0-416b-8ff0-f118bdcb4396"
      },
      "source": [
        "print(\"Training Accuracy :\", RFR_Training_Acc)\n",
        "print(\"Testing Accuracy :\", RFR_Testing_Acc)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy : 0.9330448761658652\n",
            "Testing Accuracy : 0.5204655411351908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yh4LqkRNu5u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b2f628f-5685-4928-b64e-cf7f547ac587"
      },
      "source": [
        "RFR_Train_Prediction = RFR.predict(X_train)\n",
        "print(RFR_Train_Prediction[:5])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[205.33166667 203.72       157.15222222 156.31666667 187.01333333]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKv5-Eju6pq2"
      },
      "source": [
        "RFR_Training_Err = mean_squared_error(y_train,RFR_Train_Prediction)\n",
        "RFR_Testing_Err = mean_squared_error(y_test, RFR_y_pred)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GldhMsS965mK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7592e1b-9468-4b2b-b824-c5b893399821"
      },
      "source": [
        "print(\"Training Error :\", RFR_Training_Err)\n",
        "print(\"Testing Error :\", RFR_Testing_Err)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Error : 826.5234078040801\n",
            "Testing Error : 5513.753099852634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvXL_UfIM-zi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9996f5bd-597c-4732-d6d4-31f41c3b7839"
      },
      "source": [
        "# k-fold CV \n",
        "#lm = LinearRegression()\n",
        "RFR_scores = cross_val_score(RFR, X, y, scoring='r2', cv=5)\n",
        "print(RFR_scores)\n",
        "RFR_CV_Average=np.average(RFR_scores)   \n",
        "RFR_CV_Average"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.56971743 0.516992   0.54696346 0.50862721 0.54792526]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5380450729398657"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJkGF0mTJRYj"
      },
      "source": [
        "##**3) Decision Trees Regressor (DTR)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECWWvatgJSMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eab8cef-e12c-4a2e-fa0e-cf9d40e525f6"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor \n",
        "#DecisionTreeRegressor class has many parameters. Input only random_state=0 or 42.\n",
        "DTR = DecisionTreeRegressor(random_state=0)\n",
        "#Fit the regressor object to the dataset. \n",
        "DTR.fit(X_train,y_train)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(random_state=0)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jjnIzeBJmWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a68793-640e-4806-89a6-9dd3db41dce3"
      },
      "source": [
        "DTR_y_pred = DTR.predict(X_test)\n",
        "print(y_test)\n",
        "print()\n",
        "print(DTR_y_pred)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3037    245.0\n",
            "3485    339.0\n",
            "3347     40.0\n",
            "72      200.0\n",
            "2284    200.0\n",
            "        ...  \n",
            "643      85.0\n",
            "570      68.0\n",
            "3509    175.0\n",
            "173     250.0\n",
            "1746    850.0\n",
            "Name: Price, Length: 841, dtype: float64\n",
            "\n",
            "[700.         590.          80.          85.         195.\n",
            " 100.         185.         100.          50.          90.\n",
            " 183.         200.         150.          40.         160.\n",
            "  60.          89.         139.         190.          85.\n",
            " 149.          40.         110.         103.          70.\n",
            " 129.         149.         225.          59.         600.\n",
            "  75.         120.         175.          81.         200.\n",
            "  27.          89.         115.         190.         299.\n",
            " 135.         100.          69.          92.         265.\n",
            " 169.         135.         175.         225.         220.\n",
            " 170.         250.          49.5        169.          89.\n",
            " 185.         400.         194.5        110.         250.\n",
            " 165.         225.         225.         199.         150.\n",
            " 199.          70.         112.         100.         140.\n",
            "  69.         124.          55.         215.         450.\n",
            " 400.          93.         175.         999.          80.\n",
            " 160.          90.         450.         300.         350.\n",
            " 330.          14.         200.         180.          99.\n",
            " 150.          90.         279.         205.         144.\n",
            "  75.         109.         250.         110.         120.\n",
            "  65.         195.         400.         700.          91.\n",
            "  80.         195.         100.          75.         215.\n",
            " 138.          60.          60.         170.         200.\n",
            " 120.         199.         200.         300.         100.\n",
            " 125.         250.         100.          88.         116.\n",
            " 279.         207.         165.         175.         204.5\n",
            " 165.          79.          55.         103.33333333 110.\n",
            " 350.         495.         125.          79.          98.\n",
            " 100.         124.         100.         300.         100.\n",
            " 160.         225.          36.         280.         138.\n",
            " 120.         425.          50.         115.          60.\n",
            " 110.          45.         250.          99.         135.\n",
            " 129.         175.         150.         250.         160.\n",
            " 153.         600.         160.          71.         200.\n",
            " 135.         105.         325.         140.          99.\n",
            " 189.         399.          75.         135.         125.\n",
            "  76.66666667  90.         138.          40.         125.\n",
            " 112.          98.          90.          75.         480.\n",
            " 210.         114.          45.         105.         119.\n",
            "  89.         120.          92.         412.          50.\n",
            " 395.         155.         100.         180.          70.\n",
            "  99.         105.         425.          77.5         99.\n",
            "  73.          55.         200.         115.         100.\n",
            " 350.         109.         600.         110.         450.\n",
            "  58.         250.          53.         129.         212.5\n",
            " 175.         200.         225.          80.         169.\n",
            " 150.         100.         225.         102.          95.\n",
            " 249.          88.         230.          80.          49.\n",
            "  65.         200.         249.          45.         200.\n",
            "  45.         120.         129.          69.         112.\n",
            "  65.         100.          47.          63.          74.5\n",
            " 200.         400.          99.         225.         106.\n",
            " 169.          80.          89.          90.          80.\n",
            " 249.          80.         989.         120.         160.\n",
            "  85.         149.         191.         350.          99.\n",
            " 110.         125.         270.         600.          80.\n",
            " 405.         169.          85.         190.          90.\n",
            " 309.         400.          90.          98.         112.\n",
            "  46.         400.         100.         100.          65.\n",
            " 129.         400.         550.         199.         435.\n",
            "  75.          90.         202.5         80.          75.\n",
            " 150.         250.          90.         118.          57.\n",
            " 450.          70.          60.         220.         108.33333333\n",
            " 250.         395.          85.         225.         199.\n",
            "  99.          69.         315.          78.         125.\n",
            " 234.         350.         145.         200.          45.\n",
            " 100.          80.          85.          42.         200.\n",
            "  60.         250.         125.         139.5        150.\n",
            "  60.         250.          97.         399.          75.\n",
            " 119.         245.          46.         250.          50.\n",
            "  63.         110.          90.          49.         210.\n",
            " 192.          65.         128.          99.          75.\n",
            " 250.          70.         160.         240.         250.\n",
            " 250.          75.          85.         350.          80.\n",
            " 695.         249.         120.          75.         160.\n",
            " 230.         120.          99.          69.         200.\n",
            "  60.         350.          59.5        150.         119.\n",
            " 309.          45.          45.          95.         200.\n",
            " 141.66666667 200.         200.         195.         599.\n",
            " 175.         115.         120.          65.         285.\n",
            " 200.         599.         112.          75.         138.\n",
            " 145.         110.         215.         588.         150.\n",
            " 100.         195.          60.          95.         115.\n",
            " 110.          69.          90.         119.         100.\n",
            " 500.         250.          75.          69.         450.\n",
            " 108.         100.          50.         145.         250.\n",
            "  49.         219.          85.         195.          57.\n",
            " 108.33333333  69.         270.         106.          66.5\n",
            "  80.         130.         120.         580.         250.\n",
            " 299.         115.         250.         500.          80.\n",
            " 100.          50.          75.         151.          68.\n",
            " 118.         199.         245.          60.         150.\n",
            "  88.          55.         295.          62.          98.\n",
            "  95.          99.         160.          75.         429.\n",
            " 160.         152.         299.         200.         100.\n",
            " 109.         130.          90.          30.          64.\n",
            " 175.         250.          80.         175.         180.\n",
            " 120.         100.         160.         101.         300.\n",
            " 205.          95.          77.5        310.         199.\n",
            " 200.         270.         175.          38.         150.\n",
            " 130.         120.         180.         225.         174.\n",
            " 110.         300.          70.         139.         100.\n",
            " 145.         989.         100.         700.          65.\n",
            " 190.          99.         160.         165.          90.\n",
            " 126.         144.          75.         225.         135.\n",
            " 110.         135.          53.         225.          90.\n",
            "  75.         280.          55.         174.         150.\n",
            "  79.          76.         145.         202.         135.\n",
            " 450.         600.         140.          77.5         45.\n",
            " 175.          86.85714286 205.          70.         250.\n",
            " 300.         175.         110.         129.          90.\n",
            " 109.         400.         165.         300.          90.\n",
            "  59.         145.         200.          88.          79.\n",
            " 345.          40.         230.         399.         450.\n",
            "  75.          50.          54.         120.         215.\n",
            " 100.         100.         190.         300.         130.\n",
            " 150.          60.         190.         185.          55.\n",
            " 150.          47.          90.          60.          99.\n",
            "  76.          75.         160.          94.          40.\n",
            " 250.         110.         125.         161.          55.\n",
            " 175.         135.          70.         139.5        119.\n",
            " 250.         195.         150.         250.          80.\n",
            "  45.         106.          71.5        110.         110.\n",
            "  65.         167.5         91.          65.         120.\n",
            " 300.         105.         249.          55.         140.\n",
            "  95.         170.          75.          94.          79.\n",
            " 200.          65.         100.          75.         195.\n",
            "  85.          85.          50.         120.          89.\n",
            " 150.         183.         125.          69.         450.\n",
            "  50.         200.         160.         120.          90.\n",
            "  60.          98.          80.         225.         175.\n",
            " 166.          91.         120.          85.         175.\n",
            "  55.         129.          53.         157.          99.\n",
            "  99.          65.         102.         151.         100.\n",
            "  65.         201.5        300.         590.          49.\n",
            " 100.         254.         119.          79.         202.5\n",
            " 599.         170.5         80.         250.         160.\n",
            " 119.          50.          99.         110.          99.\n",
            " 135.         120.         180.         190.          65.\n",
            " 315.          65.         101.         150.         129.\n",
            " 135.          90.         100.         110.          89.\n",
            " 120.         165.         115.          89.         372.\n",
            " 120.         140.         120.         300.         124.\n",
            " 250.          99.         100.          95.         239.\n",
            " 161.         300.          50.         160.         100.\n",
            " 279.          75.          79.         160.         195.\n",
            " 120.         156.66666667  70.         175.          45.\n",
            " 198.         138.         100.         250.         300.\n",
            " 700.         130.         119.         257.         150.\n",
            " 250.         225.         275.         200.         200.\n",
            " 156.66666667  65.          85.         100.         250.\n",
            "  63.         199.         104.          70.         120.\n",
            " 595.          49.         400.         110.         100.\n",
            "  65.         176.         300.          99.          85.\n",
            "  70.         110.         130.         100.          85.\n",
            " 159.          70.         100.         130.         250.\n",
            " 145.          58.         135.         103.33333333 160.\n",
            " 150.         149.         450.         250.         130.\n",
            " 395.          99.         125.         145.          50.\n",
            " 170.         130.         159.         170.         245.\n",
            " 201.5        420.          65.75       150.          89.\n",
            "  91.         155.         125.         141.66666667  65.\n",
            " 175.          90.          69.         160.         200.\n",
            " 240.         110.          30.         350.         250.\n",
            "  95.         110.         139.         275.         120.\n",
            " 200.         150.          53.          70.          49.5\n",
            " 125.         174.          50.         200.          62.\n",
            " 700.          75.          98.         150.         188.\n",
            " 400.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR0EA5PhJ3R5"
      },
      "source": [
        "DTR_Training_Acc = DTR.score(X_train,y_train)\n",
        "DTR_Testing_Acc = r2_score(y_test,DTR_y_pred)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2sq0TsAK3Fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aada3aa-3dff-4822-ed0b-483c928385a0"
      },
      "source": [
        "print(\"Training Accuracy :\", DTR_Training_Acc)\n",
        "print(\"Testing Accuracy :\", DTR_Testing_Acc)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy : 0.9933449709815371\n",
            "Testing Accuracy : 0.07524541047549482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn0dvWynGpgh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f216f282-4f41-49db-f728-eeb1b073cb31"
      },
      "source": [
        "DTR_Train_Prediction = DTR.predict(X_train)\n",
        "print(DTR_Train_Prediction[:5])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[200. 205. 191. 148. 185.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1T29X2tGL_k"
      },
      "source": [
        "DTR_Training_Err = mean_squared_error(y_train,DTR_Train_Prediction)\n",
        "DTR_Testing_Err = mean_squared_error(y_test, DTR_y_pred)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZtMeMrdKhU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d90ae416-ee7a-49a2-b5e8-567fc32529e6"
      },
      "source": [
        "print(\"Training Error :\", DTR_Training_Err)\n",
        "print(\"Testing Error :\", DTR_Testing_Err)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Error : 82.15259637188208\n",
            "Testing Error : 10632.955338942947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkWRrahrOBGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17aeb857-96ee-4872-f924-a984fd41f962"
      },
      "source": [
        "# k-fold CV \n",
        "#lm = LinearRegression()\n",
        "DTR_scores = cross_val_score(DTR, X, y, scoring='r2', cv=5)\n",
        "print(DTR_scores)\n",
        "DTR_CV_Average=np.average(DTR_scores)   \n",
        "DTR_CV_Average"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.1949168  0.20185225 0.10444984 0.13148566 0.05491223]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1375233543747549"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhOwxz8DHzwu"
      },
      "source": [
        "##**4) Support Vector Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py3ASvybH4Tg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97bade76-f657-4422-e593-9f44ae06351c"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "SVR = SVR()\n",
        "#SVR_regressor.fit(X_train.reshape(-1,1), y_train.reshape(-1,1))\n",
        "SVR.fit(X_train, y_train)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVR()"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGN3Vg_1IU1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853e6a90-3c40-4c6f-d842-f1dddfde4ea2"
      },
      "source": [
        "SVR_y_pred = SVR.predict(X_test)\n",
        "print(y_test)\n",
        "print()\n",
        "print(SVR_y_pred)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3037    245.0\n",
            "3485    339.0\n",
            "3347     40.0\n",
            "72      200.0\n",
            "2284    200.0\n",
            "        ...  \n",
            "643      85.0\n",
            "570      68.0\n",
            "3509    175.0\n",
            "173     250.0\n",
            "1746    850.0\n",
            "Name: Price, Length: 841, dtype: float64\n",
            "\n",
            "[245.72525704 189.02991443  98.43493779 107.6460576  139.99351777\n",
            "  91.60626605  97.53142748 136.42921157 141.71126356 138.16110713\n",
            " 131.80887324 120.47368564 133.36034114 141.8381753  137.79514721\n",
            "  91.64380222 116.71733376  94.47253817 141.86582055 139.71951375\n",
            "  96.22468577 146.43482247  93.71343989 109.19286869 182.99944246\n",
            " 104.91747989 154.02515087 152.42603877 105.78165051 221.77294886\n",
            "  95.26426192 119.29813899 187.92143011 141.3602199  246.1828543\n",
            "  86.59014699 136.91508552 118.76286881 140.21788252 153.330628\n",
            " 118.41453965  96.62788653 136.42194089 108.56866462 218.94512702\n",
            " 140.82016885 105.63480762 241.5600381  214.16696781 121.49178367\n",
            " 141.32132395 144.24027327 140.46639931 105.24680289  99.45583892\n",
            " 101.23971365 146.03595799 133.1684313  137.65005604 119.1894696\n",
            " 132.41960022  94.547698   139.03528671 220.84120706 179.58785401\n",
            " 120.69614643 141.82410544 142.10836168 133.2622462  121.22946119\n",
            " 141.11655104 117.2771203   93.19795512 166.77693878 246.8677859\n",
            " 139.87436557 131.11442614 142.45326772 237.50539558 142.07758273\n",
            " 239.06519594  88.13570136 222.13304959 142.46495505 226.47096495\n",
            " 122.01227063 186.65019628 132.45026685 159.34294656 157.0485615\n",
            " 165.40515336 246.40677731 124.71216173 137.4493731  141.19707405\n",
            " 152.11446772  90.17472425 202.06621665 120.26471127  99.23582813\n",
            " 138.15985966 187.90627449 146.65504077 214.99002093 131.10963466\n",
            " 140.51953366 147.38324407 140.43967055 106.98604031 114.18355311\n",
            " 235.02233928  98.38118789 106.05266684 120.46085747 189.84869764\n",
            " 138.55661413 100.25027419 156.99865664 140.23555177 139.85449467\n",
            " 151.9059207  189.03042905 113.47440959 140.48189787 241.82810149\n",
            " 108.13588573 160.33061577 160.76856903 119.3412098  141.48824828\n",
            "  99.47065355 142.03499398 138.91588346 137.11137043 107.73020447\n",
            " 158.45670332 175.54476201  98.77850149 104.70147893 132.01589933\n",
            " 130.80895212 121.19888433 129.29509969 182.98453378 142.92299751\n",
            " 175.86807788 137.8119812  134.83144149 141.97927879  94.8905845\n",
            " 106.26818994 203.28824221 106.04883692 139.9246448  141.79660359\n",
            " 118.19731832 138.26174442 201.5439063  172.39786797 142.73003529\n",
            " 121.0640684  167.69207624 102.40024877 111.25278815 101.95662224\n",
            " 186.79416831 141.97907794 188.42087198 149.93584675 119.95463547\n",
            " 159.72150496  94.28462587 154.26580491  93.03005554 186.47877236\n",
            "  99.87142248 186.74457488 121.1727738  138.26117511 108.74891335\n",
            "  93.26626736  93.09882751 139.97060449  98.56469256  96.71434269\n",
            " 138.84278677 105.90636953 140.18553608 160.5364173  241.95044151\n",
            " 140.04299398 142.48532791  91.49417023 188.13630069 120.91761106\n",
            " 100.54393218 120.14111509 205.56994407 190.12803382 147.90800586\n",
            " 138.62995292 137.34391479 134.08312965 130.18894029  90.42779078\n",
            " 172.95599721  95.54895095 164.88361673  97.47462686 102.08808593\n",
            " 128.85462642  90.51160561 159.77397245 120.76205077 150.18459498\n",
            " 151.36823065 151.26198449 188.3578479  132.95585436 246.73892922\n",
            " 137.86790602 189.41938073 141.76934207 217.5816022  121.24481844\n",
            " 120.20651348 154.24589566 146.71696823 150.36858406 239.91806468\n",
            " 134.87580282  91.42375766 140.03961563 139.96326079 189.19613701\n",
            " 183.59437266 138.70769255  93.52417941 154.99902281 141.65493391\n",
            " 134.29389746 132.66747147 180.48914368 139.66338292 222.90620037\n",
            " 139.77960658 142.01794241 118.98796135 140.42798372 141.8693524\n",
            "  90.47557311 142.36345597 107.67256181  97.62103251 119.80728528\n",
            " 119.97602706 141.6221568  136.38583988 143.14803135 136.94313816\n",
            " 189.30357728  91.49411435 140.40958165 189.50908449 114.51223697\n",
            " 140.2125865  139.03948701 190.02311648 100.87088699 130.89726815\n",
            " 156.54099712 110.89926286 183.97624108 186.96894039  97.99253003\n",
            " 133.06168641  96.91308608 137.88599556 241.83281583  87.94304299\n",
            " 127.05218657 141.96804609 100.05135067 101.12265866 139.86626757\n",
            " 186.77179966 190.01787303 215.49958708 106.07390371 142.10836168\n",
            " 100.3620114  214.60181723 166.57158027  95.6085564  102.04427484\n",
            " 188.41524216 140.48499182 147.90725235 241.85325738 134.4044591\n",
            "  96.16598075 129.90603009 142.14123574 160.03030679 139.23544681\n",
            " 140.50575589 188.32720631 136.38206971 142.60798217 138.75292475\n",
            " 219.01411156 141.75260264 136.38629151 127.87531078 140.15450629\n",
            " 142.17136021 189.58023936 177.64447217 179.25236188 142.13707393\n",
            " 178.19482224 136.18429277 185.43257711 136.12907654 145.51942963\n",
            " 190.53979413 173.86015066 136.46030012 153.64137199 142.35362962\n",
            "  95.64554513 110.47191715 163.95529216  89.915823   151.14854189\n",
            " 109.68788907 141.6433204  139.57971098 142.0618852  121.20739469\n",
            " 136.38143601 121.10149554 119.58788642 189.58175504 100.83405254\n",
            " 142.22242651  99.81498003  89.43429124 100.91593823  95.81746942\n",
            " 116.35064835 158.16507643 138.47563947  87.76254339 215.72833512\n",
            " 181.07362872  90.42199194 138.20362358 141.7064518  101.83128359\n",
            " 102.14719422  93.29420048 132.94399231 137.07318253 140.51240583\n",
            " 119.56270939  94.8362881   94.82937361 132.7154991  189.43056875\n",
            " 189.75771094 131.1908429  139.53009537 142.40652294 142.17941908\n",
            " 188.76752365 138.61042538 162.29399046 135.72536682 120.28484861\n",
            " 106.03962347 241.02319864 102.05625327 121.14361502  97.76016842\n",
            " 159.96008982 132.42163914 115.66151876 121.05962262 121.49642758\n",
            " 121.30762131 100.72621341 187.22289509 153.69727478 239.1008405\n",
            "  94.12730779 142.07307643 151.25846949 139.87206825 158.85477829\n",
            " 151.96341499 186.99020451  88.85784919 133.29577101 120.05239679\n",
            " 130.32815055 117.45725816 132.50043162 183.53401596 139.11470548\n",
            " 121.48284617 139.72135366 106.05266684  89.20227756 166.81410547\n",
            " 121.46355671 137.16634377 142.03974004 142.24728995  97.38444019\n",
            " 219.43938758 189.78810972 140.46060208  92.1299167  216.38634766\n",
            " 140.96872535 160.23835139 104.02249342 238.71390096  97.78096948\n",
            " 140.70971064 144.67407029 163.18163835  91.86328934  92.23424868\n",
            " 140.45590304  88.99186952 137.30276167 115.55979238  91.37980391\n",
            "  96.70829768 142.61119902 138.03508993 246.33782915 121.72072721\n",
            " 121.28695155 160.85593221 142.31425286 231.68174268  87.1865725\n",
            "  88.28570132  93.33623364  91.38217024 139.91962892 182.94535022\n",
            " 142.58601364 142.13707393 121.72238403 136.16086292 157.96657924\n",
            " 140.75705835 137.67706669 241.70183372 108.22252446 140.13862048\n",
            " 101.32851223 136.43897492 201.63277164  93.27697462 189.52271631\n",
            " 110.31053448 106.44772808 152.73443809 119.75357724 104.33194923\n",
            " 157.70864672 138.03168679 134.11302605 129.48159134  92.80808685\n",
            " 136.99354836 125.91728968 137.88768973 120.71703966 139.71773886\n",
            "  94.20318834 131.43331725 188.00826267 136.51986887 159.1194021\n",
            " 118.54276856 141.99163532  95.88616382 167.14714004 133.37739529\n",
            " 136.78168609 188.79387655 119.93618572  99.2582065  100.97450628\n",
            " 121.05405257  94.01412702 110.76373752 121.77417456 142.09449764\n",
            "  97.39820014 142.71271889 142.01378101 100.70518711 134.67931372\n",
            " 121.18093062 242.23877113 108.36129846 139.42332752  89.27227881\n",
            " 188.37702636  99.29182356  92.30981143 102.87072064 133.29989883\n",
            " 140.87865283 195.96963625 102.77659181 220.4747059  154.57122869\n",
            "  91.75117046 115.95988101 141.73723715 100.70202826 120.12460121\n",
            "  91.55295069 161.4358139   92.26308092 142.18011514 115.76718412\n",
            "  97.65336792 101.75339535 105.6047171  220.83712324 121.55261162\n",
            " 234.24700449 195.89611464 183.17772682  97.09317368 106.93521635\n",
            "  92.08008594 141.97627348 119.123645    95.65559139 188.65724938\n",
            " 118.51839721 140.03693169  91.60768912 189.25744815  90.24459707\n",
            " 112.09619227 238.96954959 119.81260316 246.43995426 136.86869775\n",
            "  92.88268882 189.25319698 160.74429017  95.02292266 141.98689445\n",
            " 186.73540917 137.89790683  89.97634364 245.83567848 234.35855034\n",
            " 159.58909957 129.18610713 141.73217358  90.2391154  166.90989991\n",
            " 134.08312965 108.05045175 100.684635   119.28767562 130.05922618\n",
            " 188.53995621  91.84444657 150.12925882 118.75457557 193.78637486\n",
            " 157.37214791 120.60466304 202.94055287  97.39927066 142.12398956\n",
            " 141.8423394  142.64361154 145.87275041 153.46757114  88.86480513\n",
            " 139.07161198 140.40539693  98.04417347 212.80319158  92.73109822\n",
            " 120.97132928 154.0460187  109.48300248 142.0618852  145.17242508\n",
            " 145.09260009 185.3345556  121.29660654 119.78752091 140.58928955\n",
            " 141.87587112  99.55437119 100.11816816 188.97268583 108.30954534\n",
            " 141.79593096 142.07587872 241.50534491 141.99816769 106.00481731\n",
            " 121.54992558 126.75677428 145.61513406 118.43761811 171.43001316\n",
            "  91.46203221 141.99100687 152.11446772 138.53472252 121.18733377\n",
            " 116.99675083 134.27783783  91.42375766  93.68639246 170.41023324\n",
            "  98.09534419 103.98990018 136.44476986 138.62475571  96.02133614\n",
            " 119.99341384 226.21455586 108.68767778 136.15869476 158.87337459\n",
            " 141.39641385 132.12553853 152.91294241 138.59920662 141.04951498\n",
            " 119.22960951  93.00405447  94.11391916 114.12421653 119.33679989\n",
            " 121.24507532 188.76283608 153.66413899 118.22422073 119.93618572\n",
            " 137.11574854 158.54754717 141.41906317 120.7419207  173.84789768\n",
            "  99.79279804 106.6069289  138.81256353 142.45814967 118.1962109\n",
            " 141.99816769 107.91059291 132.09692528 189.31262872 141.63876275\n",
            " 189.4242138  142.85877922 117.28931622  96.34509511 142.1361653\n",
            " 238.16105388 142.4629072   91.91106582 186.46918235 150.4762729\n",
            " 117.67890938 142.17434412 139.16861257 131.13710195 240.08083498\n",
            " 120.15809734 138.35472972 150.99738964 214.10858336 136.36964194\n",
            " 219.88241127  90.73292963 136.56702387 121.18601831 187.69377123\n",
            " 159.80024085 141.11938518 141.99670502 132.3217121  164.64146893\n",
            " 130.52334498 142.18935141 118.48394169  98.71603776 140.84622561\n",
            " 110.52198676 132.34654993 113.23402517 158.86077834 142.92762593\n",
            " 189.20788193 141.78708024 139.9499393  105.86873675 153.98631007\n",
            " 121.20294492 160.9507382  120.1576239  143.68534008  98.57656469\n",
            " 221.29707348 141.84740461  91.15289551 153.24417157 177.35754719\n",
            " 141.77474131 121.44146235 142.12147734 119.89396479 140.58976729\n",
            "  98.89378793 119.40035811  98.60811733 189.54964179 120.19996295\n",
            " 141.9082829  133.33708025 120.30562639 116.03760404 141.09025232\n",
            " 141.89068302 168.53584245 142.67006543 189.47760497 142.10926887\n",
            " 126.36499678 140.76054538 141.03168061 142.09956847 189.2405071\n",
            " 142.0404578  133.37739529 139.28347309 142.01378101 102.65168954\n",
            " 119.08825124 136.01154249 144.39926792 119.41597914 130.30428589\n",
            "  93.03793783 116.02121387 174.36957892 136.38193268 121.09698972\n",
            " 119.34023328 119.39092816 188.92079866 141.99670502  97.45704552\n",
            " 140.04581252 139.11100457 100.88183114  93.82670363 168.41904742\n",
            " 128.81913226 189.13969171 140.24721987 136.14462744 109.6138805\n",
            " 212.34058079 141.76967574 160.64647144 158.45614999 181.9213407\n",
            " 140.82411612 139.89362801 128.08188328 108.32970205  91.67809374\n",
            " 139.89504624 120.52106095 127.96768335 241.7826539  121.70500217\n",
            " 118.82424228 141.97000616  95.62074593  95.37092772 104.98173284\n",
            " 195.69401266 133.40773384 121.38233952 132.7557784  100.07053392\n",
            " 133.62889038 127.7408796  142.01964222 152.51742667 153.75475685\n",
            " 166.72987021 131.14000819 130.87867569 161.00257701 175.40661775\n",
            " 142.09366778 151.57460169 101.2401124   96.42648752 108.00860963\n",
            "  92.19343658 148.80033231  98.56003913 140.11686072 139.86726196\n",
            " 115.99575253 142.09449764  95.61754211 120.62215944 140.47846697\n",
            " 246.79549196 119.35598325  93.42989423 189.04263681 139.96422196\n",
            " 142.35106063]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG0GLlbCIbgG"
      },
      "source": [
        "SVR_Training_Acc = SVR.score(X_train,y_train)\n",
        "SVR_Testing_Acc = r2_score(y_test,SVR_y_pred)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnuLDw3MLDf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb144781-584d-4ec6-ed1a-d24c9e721d73"
      },
      "source": [
        "print(\"Training Accuracy :\", SVR_Training_Acc)\n",
        "print(\"Testing Accuracy :\", SVR_Testing_Acc)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy : 0.22732905167513073\n",
            "Testing Accuracy : 0.17650420516993293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iAiJXkPImoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04b1996-e5d7-45d1-c8b4-995772215255"
      },
      "source": [
        "SVR_Train_Prediction = SVR.predict(X_train)\n",
        "print(SVR_Train_Prediction[:5])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[188.52818704 138.00875434 148.19480913 121.48361896 145.15043695]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miNg7gWBJv5F"
      },
      "source": [
        "SVR_Training_Err = mean_squared_error(y_train, SVR_Train_Prediction)\n",
        "SVR_Testing_Err = mean_squared_error(y_test, SVR_y_pred)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZgKTi26Krbr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1a09aaf-8614-4cb2-c118-d6adcaab808c"
      },
      "source": [
        "print(\"Training Error :\", SVR_Training_Err)\n",
        "print(\"Testing Error :\", SVR_Testing_Err)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Error : 9538.188995105664\n",
            "Testing Error : 9468.667803787523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrYM9L56OiJ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e407ede-5cda-4c74-94c6-a3b8782a26e2"
      },
      "source": [
        "# k-fold CV \n",
        "#lm = LinearRegression()\n",
        "SVR_scores = cross_val_score(SVR, X, y, scoring='r2', cv=5)\n",
        "print(SVR_scores)\n",
        "SVR_CV_Average=np.average(SVR_scores)   \n",
        "SVR_CV_Average"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.21198843 0.18409324 0.22053564 0.20680454 0.23965204]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2126147774999983"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enddHJBIFx8k"
      },
      "source": [
        "##**5) KNN Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQymq88aFxMl"
      },
      "source": [
        "#import required packages\n",
        "from sklearn import neighbors\n",
        "from sklearn.metrics import mean_squared_error \n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9ZzIB2cF--o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92d8d88a-2312-4470-8c87-38ed49079833"
      },
      "source": [
        "rmse_val = [] #to store rmse values for different k\n",
        "for K in range(20):\n",
        "    K = K+1\n",
        "    KNN_Regressor = neighbors.KNeighborsRegressor(n_neighbors = K)\n",
        "\n",
        "    KNN_Regressor.fit(X_train, y_train)  #fit the model\n",
        "    KNN_pred=KNN_Regressor.predict(X_test) #make prediction on test set\n",
        "    KNN_error = sqrt(mean_squared_error(y_test,KNN_pred)) #calculate rmse\n",
        "    rmse_val.append(KNN_error) #store rmse values\n",
        "    print('RMSE value for k= ' , K , 'is:', KNN_error)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE value for k=  1 is: 103.62167651145197\n",
            "RMSE value for k=  2 is: 91.9693765917643\n",
            "RMSE value for k=  3 is: 88.36254648969184\n",
            "RMSE value for k=  4 is: 86.94897202740852\n",
            "RMSE value for k=  5 is: 84.65755990734037\n",
            "RMSE value for k=  6 is: 83.4696912532163\n",
            "RMSE value for k=  7 is: 83.61857597247364\n",
            "RMSE value for k=  8 is: 83.80336649579502\n",
            "RMSE value for k=  9 is: 83.5647445318713\n",
            "RMSE value for k=  10 is: 83.32971992958696\n",
            "RMSE value for k=  11 is: 83.40028160166203\n",
            "RMSE value for k=  12 is: 83.02988379643861\n",
            "RMSE value for k=  13 is: 83.00780528587968\n",
            "RMSE value for k=  14 is: 83.3046979403022\n",
            "RMSE value for k=  15 is: 82.87698802048652\n",
            "RMSE value for k=  16 is: 83.16598915761125\n",
            "RMSE value for k=  17 is: 83.291797632563\n",
            "RMSE value for k=  18 is: 83.34769724711654\n",
            "RMSE value for k=  19 is: 83.40050997296981\n",
            "RMSE value for k=  20 is: 83.37677939600128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDbUPlj9KDjj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "82614ab3-d938-4350-9202-3fa7bb222dba"
      },
      "source": [
        "#plotting the rmse values against k values\n",
        "curve = pd.DataFrame(rmse_val) #elbow curve \n",
        "curve.plot()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f0c64c9eb90>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1_XVL02ppEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a10b048-c1ed-4c67-cad3-50c2e2b13f01"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "KNNR = KNeighborsRegressor(n_neighbors=5)\n",
        "KNNR.fit(X_train, y_train)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsRegressor()"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXqHGJk0qbMR"
      },
      "source": [
        "KNN1_y_pred=KNNR.predict(X_test)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLywR2tvqr7b"
      },
      "source": [
        "KNN1_Training_Acc = KNNR.score(X_train,y_train)\n",
        "KNN1_Testing_Acc = r2_score(y_test,KNN1_y_pred)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujuzNsVJrXk0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e86361-192f-4c48-dc38-386b5ae8b177"
      },
      "source": [
        "print(\"Training Accuracy :\", KNN1_Training_Acc)\n",
        "print(\"Testing Accuracy :\", KNN1_Testing_Acc)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy : 0.615523061639218\n",
            "Testing Accuracy : 0.3766901372618784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcpa3Diirl1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e599519f-5689-4bbc-e363-54eee9d2084b"
      },
      "source": [
        "KNN1_Train_Prediction = KNNR.predict(X_train)\n",
        "print(KNN1_Train_Prediction[:5])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[192.  198.2 108.2 198.4 187.8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHlazh12sI3P"
      },
      "source": [
        "KNN1_Training_Err = mean_squared_error(y_train, KNN1_Train_Prediction)\n",
        "KNN1_Testing_Err = mean_squared_error(y_test, KNN1_y_pred)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7JARxcxsX4_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e6ecb51-5546-48ef-b9db-b6bdf09c4e09"
      },
      "source": [
        "print(\"Training Error :\", KNN1_Training_Err)\n",
        "print(\"Testing Error :\", KNN1_Testing_Err)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Error : 4746.151916666667\n",
            "Testing Error : 7166.902449464924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5vhzbUtPPQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbed824e-f4f7-475d-f14c-8781a1c808f5"
      },
      "source": [
        "# k-fold CV \n",
        "#lm = LinearRegression()\n",
        "KNNR_scores = cross_val_score(KNNR, X, y, scoring='r2', cv=5)\n",
        "print(KNNR_scores)\n",
        "KNNR_CV_Average=np.average(KNNR_scores)   \n",
        "DTR_CV_Average"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.42289928 0.38460112 0.44473276 0.35986405 0.42211528]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1375233543747549"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muNYLIZdSASG"
      },
      "source": [
        "##**6) Ridge Regressor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1lptIkdSGbp"
      },
      "source": [
        "###**a) Module Tuning for Ridge Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMp6VI9gR-wV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c34298ed-8ed8-4b4e-c394-9f67955bfc0f"
      },
      "source": [
        "alphas = [0.001, 0.01, 0.1, 1, 10]\n",
        "print('All errors are RMSE')\n",
        "print('-'*76)\n",
        "for alpha in alphas:\n",
        "    # instantiate and fit model\n",
        "    ridge = Ridge(alpha=alpha, fit_intercept=True, random_state=99)\n",
        "    ridge.fit(X_train, y_train)\n",
        "    # calculate errors\n",
        "    new_train_error1 = np.sqrt(mean_squared_error(y_train, ridge.predict(X_train)))\n",
        "    #new_validation_error1 = np.sqrt(mean_squared_error(y_validation, ridge.predict(X_validation)))\n",
        "    new_test_error1 = np.sqrt(mean_squared_error(y_test, ridge.predict(X_test)))\n",
        "    # print errors as report\n",
        "    #print('alpha: {:7} | train error: {:5} | val error: {:6} | test error: {}'.\n",
        "    print('alpha: {:7} | train error: {:5} | test error: {}'.\n",
        "          format(alpha,\n",
        "                 round(new_train_error1,3),\n",
        "                 #round(new_validation_error1,3),\n",
        "                 round(new_test_error1,3)))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All errors are RMSE\n",
            "----------------------------------------------------------------------------\n",
            "alpha:   0.001 | train error: 72.746 | test error: 71.891\n",
            "alpha:    0.01 | train error: 72.746 | test error: 71.891\n",
            "alpha:     0.1 | train error: 72.747 | test error: 71.893\n",
            "alpha:       1 | train error: 72.753 | test error: 71.904\n",
            "alpha:      10 | train error: 72.803 | test error: 71.964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83lfHuSnZQQR"
      },
      "source": [
        "###**b) Execution of Ridge Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUKg4D0mZX48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69a053c7-f5f5-4ce1-9aa8-a41fffc8196e"
      },
      "source": [
        "ridge = Ridge(alpha=10)\n",
        "ridge.fit(X_train, y_train) \n",
        "pred_train_ridge= ridge.predict(X_train)\n",
        "Ridge_Train_Acc = r2_score(y_train, pred_train_ridge)\n",
        "Ridge_Train_Err = np.sqrt(mean_squared_error(y_train,pred_train_ridge))\n",
        "print(\"Train Error : \", np.sqrt(mean_squared_error(y_train,pred_train_ridge)))\n",
        "print(\"Train Accuracy : \", r2_score(y_train, pred_train_ridge))\n",
        "pred_test_ridge= ridge.predict(X_test)\n",
        "Ridge_Test_Acc = r2_score(y_test, pred_test_ridge)\n",
        "Ridge_Test_Err = np.sqrt(mean_squared_error(y_test,pred_test_ridge))\n",
        "print(\"Test Error : \", np.sqrt(mean_squared_error(y_test,pred_test_ridge))) \n",
        "print(\"Test Accuracy : \", r2_score(y_test, pred_test_ridge))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Error :  72.80295121116889\n",
            "Train Accuracy :  0.5706350103269576\n",
            "Test Error :  71.96367470300815\n",
            "Test Accuracy :  0.5495991834990379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a0emx1EbxD0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c455890-5604-4a2c-fdcd-ececdaa1749f"
      },
      "source": [
        "# k-fold CV \n",
        "#lm = LinearRegression()\n",
        "Ridge_scores = cross_val_score(ridge, X, y, scoring='r2', cv=5)\n",
        "print(Ridge_scores)\n",
        "Ridge_CV_Average=np.average(Ridge_scores)   \n",
        "Ridge_CV_Average"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.57788548 0.508231   0.60235453 0.51940001 0.58849828]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5592738609848944"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Aq9AaNbLCg"
      },
      "source": [
        "##**7) Lasso Regressor**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB7K3b6obfCI"
      },
      "source": [
        "###**a) Module Tuning for Lasso Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73XcCetubYtI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0981a0e-4f5d-449c-99ff-3420303caf3e"
      },
      "source": [
        "alphas = [0.001, 0.01, 0.1, 1, 10]\n",
        "print('All errors are RMSE')\n",
        "print('-'*76)\n",
        "for alpha in alphas:\n",
        "    # instantiate and fit model\n",
        "    #ridge = Ridge(alpha=alpha, fit_intercept=True, random_state=99)\n",
        "    #ridge.fit(X_train, y_train)\n",
        "    lasso = Lasso(alpha=alpha, fit_intercept=True, random_state=99)\n",
        "    lasso.fit(X_train, y_train)\n",
        "    # calculate errors\n",
        "    new_train_error2 = np.sqrt(mean_squared_error(y_train, lasso.predict(X_train)))\n",
        "    #new_validation_error2 = np.sqrt(mean_squared_error(y_validation, lasso.predict(X_validation)))\n",
        "    new_test_error2 = np.sqrt(mean_squared_error(y_test, lasso.predict(X_test)))\n",
        "    # print errors as report\n",
        "    #print('alpha: {:7} | train error: {:5} | val error: {:6} | test error: {}'.\n",
        "    print('alpha: {:7} | train error: {:5} | test error: {}'. \n",
        "          format(alpha,\n",
        "                 round(new_train_error2,3),\n",
        "                 #round(new_validation_error2,3),\n",
        "                 round(new_test_error2,3)))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All errors are RMSE\n",
            "----------------------------------------------------------------------------\n",
            "alpha:   0.001 | train error: 72.746 | test error: 71.885\n",
            "alpha:    0.01 | train error: 72.75 | test error: 71.898\n",
            "alpha:     0.1 | train error: 72.839 | test error: 72.061\n",
            "alpha:       1 | train error: 73.835 | test error: 72.85\n",
            "alpha:      10 | train error: 82.458 | test error: 82.68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNAGfneRV7_c"
      },
      "source": [
        "###**b) Execution of Lasso Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE4wToXkV6-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b07675f1-7045-4246-a936-3a90f3f808f2"
      },
      "source": [
        "model_lasso = Lasso(alpha=0.1)\n",
        "model_lasso.fit(X_train, y_train) \n",
        "pred_train_lasso= model_lasso.predict(X_train)\n",
        "Lasso_Train_Acc = r2_score(y_train, pred_train_lasso)\n",
        "Lasso_Train_Err = np.sqrt(mean_squared_error(y_train,pred_train_lasso))\n",
        "print(\"Train Error : \", np.sqrt(mean_squared_error(y_train,pred_train_lasso)))\n",
        "print(\"Train Accuracy : \", r2_score(y_train, pred_train_lasso))\n",
        "\n",
        "pred_test_lasso= model_lasso.predict(X_test)\n",
        "Lasso_Test_Acc = r2_score(y_test, pred_test_lasso)\n",
        "Lasso_Test_Err = np.sqrt(mean_squared_error(y_test,pred_test_lasso))\n",
        "print(\"Test Error : \", np.sqrt(mean_squared_error(y_test,pred_test_lasso))) \n",
        "print(\"Test Accuracy : \", r2_score(y_test, pred_test_lasso))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Error :  72.83930045898697\n",
            "Train Accuracy :  0.5702061543443422\n",
            "Test Error :  72.06137981190773\n",
            "Test Accuracy :  0.5483753345279887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqey3d3PYajN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "153ea540-1d87-4fc5-8caa-d946c807c2cd"
      },
      "source": [
        "# k-fold CV \n",
        "#lm = LinearRegression()\n",
        "Lasso_scores = cross_val_score(model_lasso, X, y, scoring='r2', cv=5)\n",
        "print(Lasso_scores)\n",
        "Lasso_CV_Average=np.average(Lasso_scores)   \n",
        "Lasso_CV_Average"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.57792815 0.50752312 0.60202625 0.51744968 0.58841998]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5586694377979985"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixSylphqZGjv"
      },
      "source": [
        "##**8) ElasticNet Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYNR5q7BZHY_"
      },
      "source": [
        "###**a) Module Tuning for ElasticNet Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJHnhve2c2oM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b695405-2ef5-4a01-c798-f9a18d0c4f86"
      },
      "source": [
        "alphas = [0.001, 0.01, 0.1, 1, 10]\n",
        "print('All errors are RMSE')\n",
        "print('-'*76)\n",
        "for alpha in alphas:\n",
        "    # instantiate and fit model\n",
        "    #ridge = Ridge(alpha=alpha, fit_intercept=True, random_state=99)\n",
        "    enet  = ElasticNet(alpha=alpha, fit_intercept=True, random_state=99)\n",
        "    enet.fit(X_train, y_train)\n",
        "    # calculate errors\n",
        "    new_train_error3 = np.sqrt(mean_squared_error(y_train, enet.predict(X_train)))\n",
        "    #new_validation_error3 = np.sqrt(mean_squared_error(y_validation, enet.predict(X_validation)))\n",
        "    new_test_error3 = np.sqrt(mean_squared_error(y_test, enet.predict(X_test)))\n",
        "    # print errors as report\n",
        "    #print('alpha: {:7} | train error: {:5} | val error: {:6} | test error: {}'.\n",
        "    print('alpha: {:7} | train error: {:5} | test error: {}'.     \n",
        "          format(alpha,\n",
        "                 round(new_train_error3,3),\n",
        "                 #round(new_validation_error3,3),\n",
        "                 round(new_test_error3,3)))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All errors are RMSE\n",
            "----------------------------------------------------------------------------\n",
            "alpha:   0.001 | train error: 72.758 | test error: 71.91\n",
            "alpha:    0.01 | train error: 72.846 | test error: 72.004\n",
            "alpha:     0.1 | train error: 73.849 | test error: 72.836\n",
            "alpha:       1 | train error: 79.008 | test error: 78.502\n",
            "alpha:      10 | train error: 87.498 | test error: 88.241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut0v4iH_dKXu"
      },
      "source": [
        "###**b) Execution of ElasticNet Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KLXbsgodFfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3bd29a-9278-4e56-bd1a-9c9cb8382d1f"
      },
      "source": [
        "#Elastic Net\n",
        "model_enet = ElasticNet(alpha = 0.1)\n",
        "model_enet.fit(X_train, y_train) \n",
        "pred_train_enet= model_enet.predict(X_train)\n",
        "ElasticNet_Train_Acc = r2_score(y_train, pred_train_enet)\n",
        "ElasticNet_Train_Err = np.sqrt(mean_squared_error(y_train,pred_train_enet))\n",
        "print(\"Train Error : \", np.sqrt(mean_squared_error(y_train,pred_train_enet)))\n",
        "print(\"Train Accuracy : \", r2_score(y_train, pred_train_enet))\n",
        "\n",
        "pred_test_enet= model_enet.predict(X_test)\n",
        "ElasticNet_Test_Acc = r2_score(y_test, pred_test_enet)\n",
        "ElasticNet_Test_Err = np.sqrt(mean_squared_error(y_test,pred_test_enet))\n",
        "\n",
        "print(\"Test Error : \", np.sqrt(mean_squared_error(y_test,pred_test_enet)))\n",
        "print(\"Test Accuracy : \", r2_score(y_test, pred_test_enet))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Error :  73.84941332449763\n",
            "Train Accuracy :  0.5582030035221981\n",
            "Test Error :  72.83560506503301\n",
            "Test Accuracy :  0.538618719102927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqDyEp9bdY0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d447aa5c-160a-4d62-e547-5625d16cb57e"
      },
      "source": [
        "# k-fold CV \n",
        "#lm = LinearRegression()\n",
        "ElasticNet_scores = cross_val_score(model_enet, X, y, scoring='r2', cv=5)\n",
        "print(ElasticNet_scores)\n",
        "ElasticNet_CV_Average=np.average(ElasticNet_scores)   \n",
        "ElasticNet_CV_Average"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.56599428 0.50205348 0.59676555 0.51010951 0.56878144]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5487408519099144"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYHXBOLgXMT3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "e6a1f64e-7c45-47b0-83b9-cefd011f1373"
      },
      "source": [
        "df1 = pd.DataFrame({\n",
        "    'ML Algorithm': ['Linear_Regression','Random_Forest_Regressor','Decision_Trees_Regressor','Support_Vector_Regressor', 'KNN_Regressor', 'Ridge_Regressor','Lasso_Regressor', 'ElasticNet_Regressor'],\n",
        "    'Train_acc': [LR_Training_Acc, RFR_Training_Acc, DTR_Training_Acc, SVR_Training_Acc, KNN1_Training_Acc, Ridge_Train_Acc, Lasso_Train_Acc, ElasticNet_Train_Acc], \n",
        "    'Test_acc': [LR_Testing_Acc, RFR_Testing_Acc, DTR_Testing_Acc, SVR_Testing_Acc, KNN1_Testing_Acc, Ridge_Test_Acc, Lasso_Test_Acc, ElasticNet_Test_Acc ],\n",
        "    'Cross_Val_Score_Average': [LR_CV_Average, RFR_CV_Average, DTR_CV_Average, SVR_CV_Average, KNNR_CV_Average, Ridge_CV_Average, Lasso_CV_Average, ElasticNet_CV_Average],\n",
        "    'Train_Err': [LR_Training_Err, RFR_Training_Err, DTR_Training_Err, SVR_Training_Err, KNN1_Training_Err, Ridge_Train_Err, Lasso_Train_Err, ElasticNet_Train_Err], \n",
        "    'Test_Err': [LR_Testing_Err, RFR_Testing_Err, DTR_Testing_Err, SVR_Testing_Err, KNN1_Testing_Err, Ridge_Test_Err, Lasso_Test_Err, ElasticNet_Test_Err],\n",
        "    #'GridSearchCV_based_acc': [lr_grid_acc_avg, bayes_grid_acc_avg, rfc_grid_acc_avg, dtc_grid_acc_avg, kNN_grid_acc_avg, svc_grid_acc_avg],\n",
        "    'Suitability': ['Not Suitable', 'Not Suitable', 'Not Suitable', 'Not Suitable', 'Not Suitable', 'Finalized', 'Not Suitable', 'Not Suitable']})\n",
        "\n",
        "df1"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ML Algorithm</th>\n",
              "      <th>Train_acc</th>\n",
              "      <th>Test_acc</th>\n",
              "      <th>Cross_Val_Score_Average</th>\n",
              "      <th>Train_Err</th>\n",
              "      <th>Test_Err</th>\n",
              "      <th>Suitability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Linear_Regression</td>\n",
              "      <td>0.571301</td>\n",
              "      <td>0.550505</td>\n",
              "      <td>0.559087</td>\n",
              "      <td>5292.042321</td>\n",
              "      <td>5168.355891</td>\n",
              "      <td>Not Suitable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Random_Forest_Regressor</td>\n",
              "      <td>0.933045</td>\n",
              "      <td>0.520466</td>\n",
              "      <td>0.538045</td>\n",
              "      <td>826.523408</td>\n",
              "      <td>5513.753100</td>\n",
              "      <td>Not Suitable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Decision_Trees_Regressor</td>\n",
              "      <td>0.993345</td>\n",
              "      <td>0.075245</td>\n",
              "      <td>0.137523</td>\n",
              "      <td>82.152596</td>\n",
              "      <td>10632.955339</td>\n",
              "      <td>Not Suitable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Support_Vector_Regressor</td>\n",
              "      <td>0.227329</td>\n",
              "      <td>0.176504</td>\n",
              "      <td>0.212615</td>\n",
              "      <td>9538.188995</td>\n",
              "      <td>9468.667804</td>\n",
              "      <td>Not Suitable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KNN_Regressor</td>\n",
              "      <td>0.615523</td>\n",
              "      <td>0.376690</td>\n",
              "      <td>0.406842</td>\n",
              "      <td>4746.151917</td>\n",
              "      <td>7166.902449</td>\n",
              "      <td>Not Suitable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Ridge_Regressor</td>\n",
              "      <td>0.570635</td>\n",
              "      <td>0.549599</td>\n",
              "      <td>0.559274</td>\n",
              "      <td>72.802951</td>\n",
              "      <td>71.963675</td>\n",
              "      <td>Finalized</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Lasso_Regressor</td>\n",
              "      <td>0.570206</td>\n",
              "      <td>0.548375</td>\n",
              "      <td>0.558669</td>\n",
              "      <td>72.839300</td>\n",
              "      <td>72.061380</td>\n",
              "      <td>Not Suitable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ElasticNet_Regressor</td>\n",
              "      <td>0.558203</td>\n",
              "      <td>0.538619</td>\n",
              "      <td>0.548741</td>\n",
              "      <td>73.849413</td>\n",
              "      <td>72.835605</td>\n",
              "      <td>Not Suitable</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               ML Algorithm  Train_acc  ...      Test_Err   Suitability\n",
              "0         Linear_Regression   0.571301  ...   5168.355891  Not Suitable\n",
              "1   Random_Forest_Regressor   0.933045  ...   5513.753100  Not Suitable\n",
              "2  Decision_Trees_Regressor   0.993345  ...  10632.955339  Not Suitable\n",
              "3  Support_Vector_Regressor   0.227329  ...   9468.667804  Not Suitable\n",
              "4             KNN_Regressor   0.615523  ...   7166.902449  Not Suitable\n",
              "5           Ridge_Regressor   0.570635  ...     71.963675     Finalized\n",
              "6           Lasso_Regressor   0.570206  ...     72.061380  Not Suitable\n",
              "7      ElasticNet_Regressor   0.558203  ...     72.835605  Not Suitable\n",
              "\n",
              "[8 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    }
  ]
}